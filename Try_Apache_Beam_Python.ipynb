{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNKIMlEDZ_Vw"
   },
   "source": [
    "# Try Apache Beam - Python\n",
    "\n",
    "In this notebook, we set up the environment and walk through a simple example using Apache Beam’s DirectRunner. You will see how to prepare input data, construct a basic pipeline, and process a text file locally. Each section is designed to help you understand the fundamental steps involved in creating and running a Beam pipeline, from reading input to producing and examining output results. \n",
    "\n",
    "To run a code cell, you can click the **Run cell** button at the top left of the cell, or by select it and press **`Shift+Enter`**. Try modifying a code cell and re-running it to see what happens.\n",
    "\n",
    "To learn more about Colab, see [Welcome to Colaboratory!](https://colab.sandbox.google.com/notebooks/welcome.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz6KSQ13_3Rr"
   },
   "source": [
    "# Setup\n",
    "In this section, we prepare the environment by creating the data directory and ensuring that the input text file is available locally.\n",
    "\n",
    "This file will be used as input to the Apache Beam pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "GOOk81Jj_yUy",
    "outputId": "d283dfb2-4f51-4fec-816b-f57b0cb9b71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input file: data/prejudice.txt\n"
     ]
    }
   ],
   "source": [
    "# 1) Setup + quick sanity checks (no printing of file contents)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_file = data_dir / 'prejudice.txt'\n",
    "print(\"Using input file:\", input_file)\n",
    "\n",
    "# Ensure the file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Expected input file at {input_file} but it does not exist.\")\n",
    "\n",
    "# Optionally open the file to ensure it is readable (no printing)\n",
    "with input_file.open('r', encoding='utf-8', errors='ignore') as f:\n",
    "    first_line = f.readline()  # read one line silently just to confirm readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HubCrk-h_G"
   },
   "source": [
    "# Word count \n",
    "In this section, we build a simple Apache Beam pipeline that reads the local text file, splits it into words, and counts how many times each word appears. The results are then written to a single output file in the data/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1173
    },
    "id": "x_D7sxUHFzUp",
    "outputId": "44c926df-aa4a-4bea-9247-27c7cb537717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline finished. Output written with prefix: outputs/output_words\n"
     ]
    }
   ],
   "source": [
    "# 2) Minimal Apache Beam pipeline (wordcount) reading local file and writing local output\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "input_path = str(input_file)              \n",
    "output_prefix = 'outputs/output_words'    \n",
    "\n",
    "# Ensure outputs directory exists\n",
    "Path('outputs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Remove previous output file\n",
    "if os.path.exists(output_prefix):\n",
    "    os.remove(output_prefix)\n",
    "\n",
    "options = PipelineOptions(['--runner=DirectRunner'])\n",
    "\n",
    "def normalize_and_split(line):\n",
    "    # PRESERVE CASE — do not lowercase\n",
    "    # Keep only letters + apostrophes\n",
    "    line = re.sub(r\"[^A-Za-z']+\", ' ', line)\n",
    "    for w in line.split():\n",
    "        w = w.strip(\"'\")\n",
    "        if w:\n",
    "            yield w\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    (\n",
    "        p\n",
    "        | 'Read' >> beam.io.ReadFromText(input_path)\n",
    "        | 'Split' >> beam.FlatMap(normalize_and_split)\n",
    "        | 'PairWithOne' >> beam.Map(lambda w: (w, 1))\n",
    "        | 'Count' >> beam.CombinePerKey(sum)\n",
    "        # FORMAT AS PYTHON TUPLE STRING\n",
    "        | 'Format' >> beam.Map(lambda kv: str((kv[0], kv[1])))\n",
    "        | 'Write' >> beam.io.WriteToText(output_prefix, shard_name_template='')\n",
    "    )\n",
    "\n",
    "print(\"Pipeline finished. Output written with prefix:\", output_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Read and display top 20 words from the pipeline output\n",
    "In this section, we read the word-count results produced by the pipeline and load them into a Python Counter. We then sort the counts and display the top twenty most frequent words found in the input text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading results from: outputs/output_words\n",
      "\n",
      "Top 20 words:\n",
      "            the : 4195\n",
      "             of : 1018\n",
      "             as : 976\n",
      "            and : 968\n",
      "             to : 686\n",
      "            The : 681\n",
      "         gently : 672\n",
      "        carried : 664\n",
      "           with : 659\n",
      "        between : 657\n",
      "      Elizabeth : 654\n",
      "         slowly : 643\n",
      "             on : 623\n",
      "           calm : 621\n",
      "          Darcy : 606\n",
      "        beneath : 363\n",
      "      gathering : 363\n",
      "         clouds : 363\n",
      "  Conversations : 359\n",
      "           long : 359\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "out_path = Path('outputs/output_words')\n",
    "\n",
    "# Detect file (or shard) if exact name doesn't match\n",
    "if not out_path.exists():\n",
    "    candidates = list(Path('outputs').glob('output_words*'))\n",
    "    if candidates:\n",
    "        out_path = candidates[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find output file starting with 'outputs/output_words'\")\n",
    "\n",
    "print(\"Reading results from:\", out_path)\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "tuple_pattern = re.compile(r\"\\('(.+?)',\\s*(\\d+)\\)\")\n",
    "\n",
    "with out_path.open('r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Match tuple-style: ('WORD', 123)\n",
    "        m = tuple_pattern.match(line)\n",
    "        if m:\n",
    "            word = m.group(1)\n",
    "            count = int(m.group(2))\n",
    "            counter[word] = count\n",
    "            continue\n",
    "\n",
    "print(\"\\nTop 20 words:\")\n",
    "for word, cnt in counter.most_common(20):\n",
    "    print(f\"{word:>15} : {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Try Apache Beam - Python",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
